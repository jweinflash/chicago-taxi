```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("DBI")
library("rgdal")
source("helper.R")
```

```{r read-md-helper, include=FALSE}
knitr::read_chunk("rmd-helper.R")
```

```{r read-pickups, include=FALSE}
knitr::read_chunk("pickups.R")
```

## Processing 105 million taxi trips and visualizing it with ggmap
[Andy][andy-link] and his team had an interesting challenge for [week 6 of Makeover Monday][week-6-link]  (yes, I know I'm a few weeks behind ;). The goal was to create a visualization that showed how Chicagoans use taxis to get around the city. The challenge to me was particularly interesting because it (1) involved a large amount of data (~42GB of trip records), requiring us to be a bit careful with how we approach it, and (2) is inherently spatial in nature. I'd been meaning to play with `R`'s [ggmap][ggmap-link] package for a while now for just this sort of thing, so I thought this would be a great chance to try it out. <br><br>

#### General precursory thoughts for handling a dataset of this size
The first thing to realize about working with a dataset this big is that we don't need to load *all of it* (i.e. every record) for the analysis we're interested in doing. Rather, we just need some subset or aggregation of it. This is good, because the subset / aggregation we're interested in is likely (or more likely, at least) to fit into memory, which gives us a chance to analyze it with our usual tools. To me, the most natural thing to do in this situation is to import the data into a database, like [SQLite][sqlite-link]. Once there, we can execute queries for the particular subset / aggregation we need, and then boom, we're off to the races. There are a lot of nice tutorials about initializing the database (e.g. item 8 in the SQLite link above) and interacting with it from `R` (like [Hadley Wickham's tutorial][hadley-sqlite]), so I'm not going to spend much time working through these things in this post. <br><br>

#### Getting acquainted with the data
[The dataset that we'll be working with][taxi-data-link] is essentially a logfile of taxi trips. Each record holds information about a single trip, and contains fields like the pickup time, pickup area, dropoff time, dropoff area, total fare, and so on. Below are the first five rows and a few columns of the data to give you a sense of its structure.

```{r sample-of-raw-data, echo=FALSE}
```
<br>

#### Answering a particular question: what are the most common pickup locations in Chicago?
One angle that I was interested in exploring was to see which locations are the most popular for taxi pickups. Since the city has such cold winters, I was also curious about whether these locations remain the most popular throughout the year. We'll build up a visualization with `ggmap` and `ggplot2` to answer this. <br><br> 

#### Step 1: Querying the data
Querying the data for this is pretty simple to do. We just need to get the `COUNT` of pickups per `Pickup Community Area`, per say each quarter, so that we can study the trend over time. SQL's `GROUP BY` makes this easy to do:

```{r query-database-for-counts, eval=FALSE}
```

**Note that in the query, I'm not grouping the counts by quarter, but by month and year**. SQLite doesn't have great support for datetime data, so I figured it'd be easier to extract the counts this way first, and then use `R`'s functionality to get the quarter-based numbers we're looking for. At this point, the data in `df_pick` looks something like like this:

```{r sample-of-monthly-counts, echo=FALSE, cache=TRUE}
```

Converting to quarter-based counts is pretty straightforward -- we just replace each `YYYY-MM` date with a `YYYY-Q` date and sum up the counts belonging to the same period. I do this with the below code (which, note, converts the quarter based counts to percentages).

```{r normalize, eval=FALSE}
```

At this point, our data is structured like the small subset below. Each record represents the percentage of pickups that occured at the indicated `area_no` in that `period` (quarter) of time. We now have the "count" data we need -- the next step is to connect it with spatial information so that we can visualize it with `ggmap`.

```{r normalize-sample, echo=FALSE, dependson="sample-of-monthly-counts"}
```
<br>

#### Step 2: Loading and integrating the spatial data





[andy-link]: https://twitter.com/VizWizBI?lang=en
[week-6-link]: https://trimydata.com/2017/02/07/makeover-monday-week-6-2017-inside-chicagos-taxi-data/
[ggmap-link]: https://github.com/dkahle/ggmap
[sqlite-link]: https://www.sqlite.org/cli.html
[hadley-sqlite]: https://cran.r-project.org/web/packages/RSQLite/vignettes/RSQLite.html
[taxi-data-link]: https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew