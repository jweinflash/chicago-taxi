---
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("DBI")
library("rgdal")
source("../scripts/helper.R")
```

```{r read-md-helper, include=FALSE}
knitr::read_chunk("analysis-helper.R")
```

```{r read-pickups, include=FALSE}
knitr::read_chunk("../scripts/pickups.R")
```

```{r read-helper, include=FALSE}
knitr::read_chunk("../scripts/helper.R")
```

# Processing 105 million taxi trips and visualizing it with ggmap
[Andy][andy-link] and his team had an interesting challenge for [week 6 of Makeover Monday][week-6-link]  (yes, I know I'm a few weeks behind ;). The goal was to create a visualization that showed how Chicagoans use taxis to get around the city. The challenge to me was particularly interesting because it (1) involved a large amount of data (~42GB of trip records), requiring us to be a bit careful with how we approach it, and (2) is inherently spatial in nature. I'd been meaning to play with `R`'s [ggmap][ggmap-link] package for a while now for just this sort of thing, so I thought this would be a great chance to try it out.

#### General precursory thoughts for handling a dataset of this size
The first thing to realize about working with a dataset this big is that we don't need to load *all of it* (i.e. every record) for the analysis we're interested in doing. Rather, we just need some subset or aggregation of it. This is good, because the subset / aggregation we're interested in is likely (or more likely, at least) to fit into memory, which gives us a chance to analyze it with our usual tools. To me, the most natural thing to do in this situation is to import the data into a database, like [SQLite][sqlite-link]. Once there, we can execute queries for the particular subset / aggregation we need, and then boom, we're off to the races. There are a lot of nice tutorials about initializing the database (e.g. item 8 in the SQLite link above) and interacting with it from `R` (like [Hadley Wickham's tutorial][hadley-sqlite]), so I'm not going to spend much time working through these things in this post.

#### Getting acquainted with the data
[The dataset that we'll be working with][taxi-data-link] is essentially a logfile of taxi trips. Each record holds information about a single trip, and contains fields like the pickup time, pickup area, dropoff time, dropoff area, total fare, and so on. Below are the first five rows and a few columns of the data to give you a sense of its structure.

```{r sample-of-raw-data-md, echo=FALSE}
```

#### Answering a particular question: what are the most common pickup locations in Chicago?
One angle that I was interested in exploring was to see which locations are the most popular for taxi pickups. Since the city has such cold winters, I was also curious about whether these locations remain the most popular throughout the year. We'll build up a visualization with `ggmap` and `ggplot2` to answer this. 

#### Step 1: Querying the data
Querying the data for this is pretty simple to do. We just need to get the `COUNT` of pickups per `Pickup Community Area`, per say each quarter, so that we can study the trend over time. SQL's `GROUP BY` makes this easy to do:

```{r query-database-for-counts, eval=FALSE}
```

**Note that in the query, I'm not grouping the counts by quarter, but by month and year**. SQLite doesn't have great support for datetime data, so I figured it'd be easier to extract the counts this way first, and then use `R`'s functionality to get the quarter-based numbers we're looking for. At this point, the data in `df_pick` looks something like like this:

```{r sample-of-monthly-counts-md, echo=FALSE, cache=TRUE}
```

Converting to quarter-based counts is pretty straightforward -- we just replace each `YYYY-MM` date with a `YYYY-Q` date and sum up the counts belonging to the same period. I do this with the below code (which, note, converts the quarter based counts to percentages).

```{r normalize, eval=FALSE}
```

At this point, our data is structured like the small subset below. Each record represents the percentage of pickups that occured at the indicated `area_no` in that `period` (quarter) of time. We now have the "count" data we need -- the next step is to connect it with spatial information so that we can visualize it with `ggmap`.

```{r normalize-md, echo=FALSE, dependson="sample-of-monthly-counts"}
```

#### Step 2: Loading and integrating the spatial data
Spatial information for the city of Chicago can be downloaded [here][spatial-data-link] (be sure to download it as a `shapefile`). Before we get into the details of this data, just know that it contains boundary information for each of the `Pickup Community Areas` we selected in our query. This boundary information is important; we'll need it to demarcate each of the communities in our ggmap.

The `shapefile` that we downloaded isn't actually a single file, but a zip containing four files: a `.shp`, `.shx`, `.dbf` and `.prj`. The `.shp` is the most important, as it contains the actual "geometry" (i.e. outline) of the communities. The others aren't as important, but if you're interested, you can see the Wikipedia page on the file structure [here][shp-wiki-link].

```{r load-spatial-data-md, results="hide", cache=TRUE}
```

The data structure that we loaded looks a bit nasty at first glance:
```{r str-spatial-data-md, dependson="load-spatial-data-md"}
```

But it actually has a fairly reasonable structure. We'll make use of two slots from this object:  

  1. **The `data` slot**. It's of class `data.frame` and holds information about each community's geographic information, like its `shape_area` and `shape_len`. Note it has 77 rows, one for each community.
  2. **The `polygons` slot**. It's of class `list` and holds `Polygon` objects. Each `Polygon` contains a `coords` matrix that lists the (longitude, latitude) pairs that trace its boundary. Note that there are 77 of these as well; this is the case because each one corresponds to a community from `data`. They match in terms of offset, so the first community in `data` maps to the first `Polygon`, the second community to the second `Polygon`, and so forth.
  
We need to extract the `coords` information from each `Polygon` and link it to the community it represents so that we can draw them properly on our map. I do this with the `extract_community_area_data` function below. **Note that if you're not interested in munging around with these data structures,  `ggplot2`'s `fortify` function will give you just about the same output!**

```{r extract-spatial-data, eval=FALSE}
```
  
Running this function on `sp_comm` gives us the following output:

```{r str-extracted-spatial-data-md, dependson="load-spatial-data-md"}
```
Nice! We now have the spatial data in a form that'll be easy to integrate with the count data we queried in step one.

[andy-link]: https://twitter.com/VizWizBI?lang=en
[week-6-link]: https://trimydata.com/2017/02/07/makeover-monday-week-6-2017-inside-chicagos-taxi-data/
[ggmap-link]: https://github.com/dkahle/ggmap
[sqlite-link]: https://www.sqlite.org/cli.html
[hadley-sqlite]: https://cran.r-project.org/web/packages/RSQLite/vignettes/RSQLite.html
[taxi-data-link]: https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew
[spatial-data-link]: https://data.cityofchicago.org/Facilities-Geographic-Boundaries/Boundaries-Community-Areas-current-/cauq-8yn6
[shp-wiki-link]: https://en.wikipedia.org/wiki/Shapefile